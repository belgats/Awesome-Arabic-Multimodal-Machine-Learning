# Awesome-Arabic-Multimodal-Machine-Learning

This repository is a curated list of resources related to Arabic multimodal machine learning, including papers, datasets, and models.
As a part of this release we share the information about recent multimodal models and datasets which are available for research purposes.
We found that although N+ multimodal language resources are available in literature for various applications. 
## Table of Contents
- [Datasets](#datasets)
- [Papers](#papers)
- [Models](#models)

## Datasets
A list of datasets for Arabic multimodal machine learning.

| **Dataset**       | **Title of the Paper** | **Link of the Paper** | **Link of the Dataset** | **Application/Features** | **Availability** |
| ----------------- | ---------------------- |---------------------- |------------------------ | ------------------------ | ---------------- |
| IEMOCAP| IEMOCAP: interactive emotional dyadic motion capture database | [Paper](https://link.springer.com/content/pdf/10.1007/s10579-008-9076-6.pdf) | [Dataset](https://sail.usc.edu/software/databases/) | Emotion recognition | ❌ |
| MMAC (2010)       | MMAC | [Paper](#) | [Dataset](#) | OCR development, linguistic research | ❌ |
| AVAS (2013)       | AVAS | [Paper](#) | [Dataset](#) | Based on AVSR, lip-reading, Face Recognition under varying conditions | ❌ |
| AMMD (2019)       | AMMD | [Paper](#) | [Dataset](#) | Sentiment Analysis | ❌ |
| AVANEmo (2019)    | AVANEmo | [Paper](#) | [Dataset](#) | Emotion Recognition (6 basic emotions) | ❓ |
| Bellagha et Zrigui (2021) | Bellagha et Zrigui | [Paper](#) | [Dataset](#) | Speaker Role Recognition in TV broadcasts | ✔️ |
| Albalawi et al. (2022) | Albalawi et al. | [Paper](#) | [Dataset](#) | Rumor Detection | ❓ |
| ArabSign (2023)   | ArabSign | [Paper](#) | [Dataset](#) | Arabic Sign Language (ArSL) recognition | ✔️ |
| AMSA (2023)       | AMSA | [Paper](#) | [Dataset](#) | Sentiment Analysis | ✔️ |
| Abbas et al. (2024) | Abbas et al. | [Paper](#) | [Dataset](#) | Religious speech, ArSL annotation | ❓ |
| Alroken (2023)    | Alroken | [Paper](#) | [Dataset](#) | [Description needed] | ❌ |
| L2-KSU (2025)     | L2-KSU | [Paper](#) | [Dataset](#) | [Description needed] | ❓ |
| CAMEL-Bench (2025) | CAMEL-Bench | [Paper](#) | [Dataset](#) | 45 Visual QA datasets, available at [HuggingFace](https://huggingface.co/collections/ahmedheakl/camel-bench-670750f3998395452cd3b7b1) | ✔️ |

## Papers
A list of research papers on Arabic multimodal machine learning.

| Title | Authors | Year | Link |
|-------|---------|------|------|
| Paper 1 | Author 1 | 2020 | [Link](#) |


## Models
A list of models for Arabic multimodal machine learning, organized by modality and criteria.

### Unimodal Models
#### Text
| **Model** | **Description** | **Link** |
|-----------|-----------------|----------|
| AraBERT   | A pre-trained BERT model for Arabic text. | [Hugging Face](https://huggingface.co/aubmindlab/bert-base-arabertv02) |
| AraGPT2   | A pre-trained GPT-2 model for Arabic text generation. | [Hugging Face](https://huggingface.co/aubmindlab/aragpt2-base) |

#### Audio
| **Model** | **Description** | **Link** |
|-----------|-----------------|----------|
| Wav2Vec2-Arabic | A pre-trained Wav2Vec2 model for Arabic speech recognition. | [Hugging Face](https://huggingface.co/elgeish/wav2vec2-large-xlsr-53-arabic) |

#### Image
| **Model** | **Description** | **Link** |
|-----------|-----------------|----------|
| Arabic OCR | A pre-trained model for Arabic optical character recognition. | [Hugging Face](https://huggingface.co/arabic-ocr) |

### Multimodal Models
| **Model** | **Description** | **Link** |
|-----------|-----------------|----------|
| AraMMT    | A multimodal transformer model for Arabic text and image tasks. | [Hugging Face](https://huggingface.co/arabic-mmt) |
| Arabic-VQA | A model for visual question answering in Arabic. | [Hugging Face](https://huggingface.co/arabic-vqa) |

## Contributing
Contributions are welcome! Please read the [contributing guidelines](CONTRIBUTING.md) first.

## License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.